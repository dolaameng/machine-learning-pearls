{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probabilistic Data Structure for Mining of Massive Dataset\n",
    "- [resources](https://www.safaribooksonline.com/oriole/probabilistic-data-structures-in-python)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What it is\n",
    "\n",
    "### Probabilistic Data Strucuture usually for Approximation Algorithms\n",
    "\n",
    "### Examples of algorithms (ordered in complexity progress?)\n",
    "<table>\n",
    "<tr>\n",
    "    <th>Algorithm</th>\n",
    "    <th>usage</th>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>HyperLogLog</td><td>set cardinality</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>BloomFilter</td><td>set membership</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>MinHash</td><td>set similiarity</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>Count-Min Sketch</td><td>frequencyt summary</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>t-Digest</td><td>streaming quantiles</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "jabber_text = \"\"\"\n",
    "`Twas brillig, and the slithy toves\n",
    "  Did gyre and gimble in the wabe:\n",
    "All mimsy were the borogoves,\n",
    "  And the mome raths outgrabe.\n",
    "\n",
    "\"Beware the Jabberwock, my son!\n",
    "  The jaws that bite, the claws that catch!\n",
    "Beware the Jubjub bird, and shun\n",
    "  The frumious Bandersnatch!\"\n",
    "He took his vorpal sword in hand:\n",
    "  Long time the manxome foe he sought --\n",
    "So rested he by the Tumtum tree,\n",
    "  And stood awhile in thought.\n",
    "And, as in uffish thought he stood,\n",
    "  The Jabberwock, with eyes of flame,\n",
    "Came whiffling through the tulgey wood,\n",
    "  And burbled as it came!\n",
    "One, two! One, two! And through and through\n",
    "  The vorpal blade went snicker-snack!\n",
    "He left it dead, and with its head\n",
    "  He went galumphing back.\n",
    "\"And, has thou slain the Jabberwock?\n",
    "  Come to my arms, my beamish boy!\n",
    "O frabjous day! Callooh! Callay!'\n",
    "  He chortled in his joy.\n",
    "\n",
    "`Twas brillig, and the slithy toves\n",
    "  Did gyre and gimble in the wabe;\n",
    "All mimsy were the borogoves,\n",
    "  And the mome raths outgrabe.\n",
    "\"\"\"\n",
    "\n",
    "parker_text = \"\"\"\n",
    "My answers are inadequate\n",
    "To those demanding day and date\n",
    "And ever set a tiny shock\n",
    "Through strangers asking what's o'clock;\n",
    "Whose days are spent in whittling rhyme-\n",
    "What's time to her, or she to Time?\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "pat_word = re.compile(r\"\\w+\")\n",
    "jabber_words = [w.lower() for w in pat_word.findall(jabber_text)]\n",
    "parker_words = [w.lower() for w in pat_word.findall(parker_text)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HyperLogLog for counting unique elements\n",
    "- [github](https://github.com/svpcom/hyperloglog)\n",
    "- [wiki](https://en.wikipedia.org/wiki/HyperLogLog)\n",
    "- key idea: use hash to distribute items to uniform distribution, and estimate the set size with max leading zeros in the binary representation\n",
    "- usage:\n",
    "    - set the acceptable error and do the approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hll set size vs real size 90 91\n"
     ]
    }
   ],
   "source": [
    "import hyperloglog\n",
    "\n",
    "# accept error rate of 1%\n",
    "hll = hyperloglog.HyperLogLog(0.01)\n",
    "\n",
    "for word in jabber_words:\n",
    "    hll.add(word)\n",
    "    \n",
    "print (\"hll set size vs real size\", len(hll), len(set(jabber_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bloomfilter for searching a set\n",
    "- [github](https://github.com/jaybaird/python-bloomfilter)\n",
    "- [wiki](https://en.wikipedia.org/wiki/Bloom_filter)\n",
    "- key idea\n",
    "    - use k hash function to hash elements into the same hash array\n",
    "    - check if an element is in hash by checking all k bits are on\n",
    "    - there won't be false negative\n",
    "    - but it could have false positive when all bits are turned on by other elements\n",
    "    - it is a quick way to implement cashing before an expensive database query\n",
    "    - it is O(n) to implement \"intersection\" of two sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** pybloom is currently only supported on python2.x so I change to `bloom_filter`***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estimate vs real intersection size 8 8\n"
     ]
    }
   ],
   "source": [
    "## find the intersection of jabber_words and parker_words\n",
    "\n",
    "from bloom_filter import BloomFilter\n",
    "\n",
    "bf = BloomFilter(max_elements=10000, error_rate=0.01)\n",
    "\n",
    "for w in jabber_words:\n",
    "    bf.add(w)\n",
    "    \n",
    "common = set()\n",
    "for w in parker_words:\n",
    "    if w in bf:\n",
    "        common.add(w)\n",
    "        \n",
    "print(\"estimate vs real intersection size\", len(common), len(set(jabber_words) & set(parker_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Min-hash: Min-wise independent permutations (locality sensitive hashing) for Jaccard\n",
    "- [github](https://github.com/ekzhu/datasketch)\n",
    "- [wiki](https://en.wikipedia.org/wiki/MinHash)\n",
    "- it approximates Jaccard similiarity\n",
    "    $J = \\frac {|A \\cap B|}{|A \\cup B|}$\n",
    "- it also uses k hash functions\n",
    "- usage:\n",
    "    - it calculates `digest` (figerprint) of sets\n",
    "    - based on figerprints the similarity can be calculated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estimated vs actual 0.060546875 0.06956521739130435\n"
     ]
    }
   ],
   "source": [
    "from hashlib import sha1\n",
    "from datasketch import MinHash\n",
    "\n",
    "def mh_digest(s):\n",
    "    m = MinHash(num_perm=512) # memory allocation\n",
    "    for x in s:\n",
    "        m.update(x.encode('utf8'))\n",
    "    return m\n",
    "\n",
    "jabber_digest = mh_digest(jabber_words)\n",
    "parker_digest = mh_digest(parker_words)\n",
    "\n",
    "inter = len(set(jabber_words) & set(parker_words))\n",
    "union = len(set(jabber_words) | set(parker_words))\n",
    "\n",
    "print(\"estimated vs actual\", jabber_digest.jaccard(parker_digest),\n",
    "                            inter / union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(str, bytes)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 'hello'\n",
    "type(s), type(s.encode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## count-min sketch for frequency count\n",
    "- [github](https://github.com/IsaacHaze/countminsketch)\n",
    "- [wiki](https://en.wikipedia.org/wiki/Count%E2%80%93min_sketch)\n",
    "- key idea: \n",
    "    - similiar data structure (k hash) as used in bloom filter\n",
    "    - it is like a frequency table, even for streaming data, so conceptually it should look like a Counter in python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (countminsketch.py, line 56)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/Users/pmal252d/anaconda3/lib/python3.6/site-packages/countminsketch.py\"\u001b[0;36m, line \u001b[0;32m56\u001b[0m\n\u001b[0;31m    print hash((x, s))\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import countminsketch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## t-digest for streaming quantiles\n",
    "- [github](https://github.com/trademob/t-digest)\n",
    "- [wiki](https://github.com/tdunning/t-digest/blob/master/docs/t-digest-paper/histo.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05 estimate vs real 0.03576623657392189 0.0363860768755\n",
      "0.5 estimate vs real 0.49868795164865265 0.499089434151\n",
      "0.95 estimate vs real 0.9497762058615382 0.948161951597\n"
     ]
    }
   ],
   "source": [
    "## generate random numbers and look at its head, median and tail\n",
    "from tdigest import TDigest\n",
    "import random\n",
    "\n",
    "random.seed(314)\n",
    "\n",
    "td = TDigest()\n",
    "\n",
    "xs = []\n",
    "\n",
    "for _ in range(1000):\n",
    "    x = random.random()\n",
    "    td.add(x, 1) # put weight as 1\n",
    "    xs.append(x)\n",
    "    \n",
    "xs = np.asarray(xs)\n",
    "    \n",
    "for q in [0.05, 0.5, 0.95]:\n",
    "    print(q, \"estimate vs real\", td.quantile(q), np.percentile(xs, q*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** One of the most valuable things about probabilistic data strucutre is that they kinda \"invert\" the modelling process, i.e., they allow the user to specify the error bounds, and do the calculation online for stream data. By this way, we don't have the \"batch windows\" anymore, i.e., we don't have to collect data, fit the model and do inference in batches from time to time.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
